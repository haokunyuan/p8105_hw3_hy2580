---
title: "p8105_hw2_hy2580"
author: "haokun yuan"
date: "10/10/2018"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(p8105.datasets)
library(patchwork)
library(ggridges)
```

##Problem 1

Import and clean data
```{r}
brfss_overallhealth = brfss_smart2010 %>%
  janitor::clean_names() %>% 
  # focused on Overall Health topic
  # checked response with distinct(), no pre-collapsed categories
  filter(topic == "Overall Health") %>% 
  # take levels form excellent to poor by factorize response
  mutate(response = fct_relevel(response,c("Excellent","Very good","Good","Fair","Poor")))
```

###1.1
Find states were observed at 7 locations in 2002
```{r}
brfss_overallhealth %>% 
  filter(year == 2002) %>%
  distinct(locationabbr,locationdesc) %>% 
  count(locationabbr) %>% 
  filter(n == 7)
```

CT, FL, NC were observed at 7 locations in 2002.

###1.2
Make a spaghetti plot for number of observations in each state from 2002 to 2010
```{r}
n_observations =
  brfss_overallhealth %>% 
  group_by(year,locationabbr) %>% 
  count(locationabbr)

n_observations %>%   
  ggplot(aes( x = year, y = n, color = locationabbr)) +
  geom_line() + 
  labs(x = "Year", y = "Number of Observations", title = "Number of Observations in States (U.S.)", subtitle = "(2002-2010)")

n_observations %>% 
  arrange(desc(n))
```
As shown above, FL is most observed in 2007 and 2010, is it likely some event occured in those two years that make the data more accessible. One the other hand, NJ appears to be more observed during the entire observation period (2002-2010), which may be a result of more healthcare facilities within the state. 

###1.3

Make a table showing, for the years 2002, 2006, and 2010, the mean and standard deviation of the proportion of “Excellent” responses across locations in NY State.

```{r}
brfss_overallhealth %>% 
  filter(year %in% c(2002, 2006, 2010),
         response == "Excellent",
         locationabbr == "NY") %>%
  group_by(year) %>%
  summarise(mean(data_value),
            sd(data_value)) %>% 
  knitr::kable(digits = 2,
               col.names = c("Year", "Mean", "Standard Deviation"))
```

As the table shown above, Year 2002 appears to have the largest mean and standard deviation of the proportion of "Excellent" response across locations in NY state. While Year 2006 have the smallest mean and Year 2010 has the smallest standard deviation. 

###1.4

For each year and state, compute the average proportion in each response category (taking the average across locations in a state). Make a five-panel plot that shows, for each response category separately, the distribution of these state-level averages over time.

```{r}
brfss_overallhealth %>%
  group_by(locationabbr, year, response) %>% 
  summarize(avg_prop = mean(data_value)) %>% 
  ggplot(aes(x = year, avg_prop, color = response)) + 
  geom_point() +
  facet_grid(~ response) +
  labs(title = "State-level Health Response over 2002 to 2010",
         x = "Year",
         y = "Average Proportion by State") +
  theme(axis.text.x = element_text(angle = 45))
```
The distribution of each response across years appears to be concentrate. And the proportion of responses different by the level of response. The most likely response is "Very Good", while the least likely response is "Poor".

##Problem2
This problem uses the Instacart data. DO NOT include this dataset in your local data directory; instead, load the data from the p8105.datasets package (it’s called instacart).

The goal is to do some exploration of this dataset. To that end, write a short description of the dataset, noting the size and structure of the data, describing some key variables, and giving illstrative examples of observations. Then, do or answer the following (commenting on the results of each):
```{r}
instacart_tidy = instacart %>%
  janitor::clean_names() %>% 
  na.omit()
```

###2.1
How many aisles are there, and which aisles are the most items ordered from?

```{r}
instacart_tidy %>% 
  distinct(aisle) %>% 
  nrow()

instacart_tidy %>% 
  count(aisle) %>% 
  arrange(desc(n))
```
There are 134 aisles, and the aisles that has most items ordered from is listed in the table above. From the table, we know that fresh vegetables are the most ordered, and fresh fruits is the second, which indicate customers are more willing to order fresh producs from instacart.

###2.2

Make a plot that shows the number of items ordered in each aisle. Order aisles sensibly, and organize your plot so others can read it.

```{r}
instacart_tidy %>% 
  count(aisle) %>%
  ggplot(aes(x = reorder(aisle, -n, sum), n)) +
  geom_point() +
  theme(axis.text.x = element_text(size = 6, angle = 45, hjust = 1)) +
  labs(x = "Aisle Name",
       y = "Number of Items Ordered",
       title = "Number of Items Ordered in each Aisle" )
```
The plot above showed the number of items ordered in each aisle from a descending order. 

###2.3

Make a table showing the most popular item in each of the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”.

```{r}
instacart_tidy %>% 
  filter(aisle %in% c("baking ingredients","dog food care","packaged vegetables fruits")) %>% 
  group_by(aisle) %>% 
  count(product_name) %>% 
  top_n(1,n) %>% 
  knitr::kable(col.names = c("Aisle", "Most Order Item", "Ordered Number"))
```

Lighter brown sugar was the most ordered item among baking ingredients, which had 499 orders, Snack Sticks Chicken & Rice Recipe Dog Treats is the most ordered dog food care with 30 orders, Orgainic Baby Spinach had 9784 orders and listed as the most ordered item in packaged vegetables fruits.
ALthough those three product are most ordered item at their aisle, the ordered number is significantly different.

###2.4
Make a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week; format this table for human readers (i.e. produce a 2 x 7 table).

```{r}
instacart_tidy %>%
  filter(product_name %in% c("Pink Lady Apples","Coffee Ice Cream")) %>% 
  mutate(order_dow = 
           factor(order_dow, levels = 0:6, labels = c("Sunday", "Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday"))) %>% 
  group_by(product_name, order_dow) %>% 
  summarize(mean = mean(order_hour_of_day)) %>% 
  spread(order_dow, mean) %>% 
  knitr::kable(digits = 2,
               col.names = c("Product Name", "Sunday", "Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday"))
```

The table above showed the mean hour of the day at which the Pink Lady Apples and Coffee Ice Creams are ordered. we can see that most orders were ordered betweem 11 to 15.4. And this number may indicate a time that those products need refill.

##Problem 3

This problem uses the NY NOAA data. DO NOT include this dataset in your local data directory; instead, load the data from the p8105.datasets package (it’s called ny_noaa).

The goal is to do some exploration of this dataset. To that end, write a short description of the dataset, noting the size and structure of the data, describing some key variables, and indicating the extent to which missing data is an issue. Then, do or answer the following (commenting on the results of each):


```{r}
ny_noaa_tidy = ny_noaa %>%
  janitor::clean_names() %>% 
  separate("date", c("year", "month", "day"), sep = "-") %>% 
  mutate(tmax = as.numeric(tmax)/10,
         tmin = as.numeric(tmin)/10,
         day = as.numeric(day),
         month = factor(month.abb[as.numeric(month)], level = month.abb),
         prcp = prcp/10)
```

###3.1

Do some data cleaning. Create separate variables for year, month, and day. Ensure observations for temperature, precipitation, and snowfall are given in reasonable units. For snowfall, what are the most commonly observed values? Why?
```{r}
ny_noaa_tidy %>% 
  count(snow) %>%
  top_n(1,n)
```
The most observed value of snowfall is zero, unlike rainfall, snow falls only under relatively cold weather.

###3.2

Make a two-panel plot showing the average max temperature in January and in July in each station across years. Is there any observable / interpretable structure? Any outliers?

```{r}
ny_noaa_tidy %>% 
  filter(month %in% c("Jan", "Jul")) %>% 
  group_by(id, month,year) %>%
  summarize(mean = mean(tmax,na.rm = TRUE)) %>% 
  ggplot(aes(year,mean, group = year)) +
  geom_boxplot() +
  facet_grid(~ month) +
  labs(x = "Average Max Temperature", y = "Year", title = "Average Maximum Temperature in January and July in NY from 1980 to 2010")
```

The maximum temperature in July is significantly higher than that of January. And there are few outliers in both month.

###3.3
Make a two-panel plot showing (i) tmax vs tmin for the full dataset (note that a scatterplot may not be the best option);

```{r}
plot_temp = ny_noaa_tidy %>% 
  filter(is.na(tmax) == FALSE, is.na(tmin) == FALSE) %>% 
  ggplot(aes(x = tmax, y = tmin)) + 
  geom_hex() +
  labs(x = "Maximum Temperature(°C)",
       y = "Minimum Temperature(°C)",
       title = "Maximum vs. Minimum Temperature")
```

(ii) make a plot showing the distribution of snowfall values greater than 0 and less than 100 separately by year.

```{r}
plot_snow = ny_noaa_tidy %>%
  filter(snow > 0, snow < 100) %>%
  ggplot(aes(x = year, y = snow, group = year)) +
  geom_boxplot() +
  labs(x = "Year", 
       y = "Distribution of Snowfall(mm)",
       title = "Distribution of Snowfall") +
   theme(axis.text.x = element_text(angle = 45))
```

```{r}
plot_snow + plot_temp
```

Above plot showed the distribution of snowfall and the Maximum vs. Minimum temperature for the full dataset. The distribution of snowfall doesn't varies much by year. and the outliers in two plots matches each other which indicate that there are some extremely cold year that cause more snowfall.

The ny noaa data has `r ncol(ny_noaa_tidy)`rows and `r nrow(ny_noaa_tidy)`columns, they key variables are snowfall, precipitation, maximum temperature and minimum temperature. The data is structure so that we have the observations of variables at each observation points. The missing datas will have more effect on variables like snowfall which has little observations greater than zero.

